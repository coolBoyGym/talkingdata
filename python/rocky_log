concat_1_gblinear
alpha 0 lambda 7.5  2.3499658672 2.38944255486
concat_1_gbtree
max_depth 7 eta 0.07 subsample 0.8 colsample_bytree 0.5 2.35875357389 2.39043890146

concat_2_gblinear
alpha 0.001 lambda 8 2.34606485552 2.38613098915
concat_2_gbtree
max_depth 3 eta 0.07 subsample 0.8 colsample_bytree 0.6 2.33399659152 2.38934798614

concat_2_norm_gblinear
alpha 0.001 lambda 7.5 2.34475563006 2.38551132474
concat_2_norm_gbtree
max_depth 3 eta 0.07 subsample 0.8 colsample_bytree 0.6 2.33028431943 2.38669627018

concat_3_gblinear
alpha 0.01 lambda 10 2.32916788862 2.40911127508
concat_3_gbtree
max_depth 4 eta 0.05 subsample 0.7 colsample_bytree 0.6 2.22950330251 2.38446315655

concat_3_norm_gbtree
max_depth 4 eta 0.05 subsample 0.7 colsample_bytree 0.6 2.23408867668 2.38441965457
concat_3_norm_gblinear
alpha 0 lambda 10 2.32955361167 2.40859080021


adadelta learning_rate=0.3
[304]	train_score: 2.208679	valid_score: 2.259315

(100,10) lr=0.5
[137]	train_score: 2.369727	valid_score: 2.365398
(100,20) lr=0.5
[141]	train_score: 2.362218	valid_score: 2.358099
(100,50) lr=0.5
[181]	train_score: 2.322096	valid_score: 2.328415
(100,100) lr=0.5
[158]	train_score: 2.306684	valid_score: 2.312795
(100,80) lr=0.5
[362]	train_score: 2.320007	valid_score: 2.329979
(100,120) lr=0.5
[348]	train_score: 2.312351	valid_score: 2.321759
(100,150) lr=0.5
[391]	train_score: 2.284677	valid_score: 2.295818

first layer 800
second layer 500, 800, 1500, 2000, 3000
lr=0.5
差别不大 均在2.275附近，曲线也近似



concat_7_norm gbtree
max_depth 7 eta 0.05 subsample 0.8 colsample_bytree 0.5 1.89917527015 2.27997448448

500, lr=0.2
[364]	train_score: 2.195357	valid_score: 2.261290



feature from leaderboard
gbtree with experienced parameter
score=2.626



concat_6
hidden layer=128    [468]	train_score: 2.200564	valid_score: 2.258902  score=2.24487
hidden layer=64     [530]	train_score: 2.207147	valid_score: 2.258491  score=2.24418
hidden layer=64     [470]   score=2.24484


8.20
concat_20
    gblinear
        experienced parameter (0,10)            {train_logloss=2.0307, valid_logloss=2.3094}
    gbtree
        experienced parameter (0.1,3,0.8,0.6)   {train_logloss=2.0519, valid_logloss=2.2807}
    mlp
        experienced parameter (0.2, 64)         {[441]	train_score: 2.205067	valid_score: 2.261022}
concat_21
    gblinear
        experienced parameter (0,10)            {[2]	train-mlogloss:2.072748	eval-mlogloss:2.300467}
    gbtree
        experienced parameter (0.1,3,0.8,0.6)   {[625]	train-mlogloss:2.070436	eval-mlogloss:2.285588}
    mlp
        experienced parameter (0.2, 64)         {[491]	train_score: 2.211974	valid_score: 2.259662}

concat_1:
    mlp
        experienced parameter (0.2,64)          {[2367]	train_score: 2.370186	valid_score: 2.387985}   concat_1_multi_layer_perceptron_1.bin
        (0.2, 128)                              {[2046]	train_score: 2.368369	valid_score: 2.388239}
        (0.2, 32)                               {[2338]	train_score: 2.377771	valid_score: 2.388956}
        (0.1, 64 ,64)                           {[24]	train_score: 2.378819	valid_score: 2.388655}
        (0.1, 64, 128)                          {[20]	train_score: 2.377966	valid_score: 2.388651}

embedding:
    phone_brand:
        (0.2, 64)       {[2121]	train_score: 2.402465	valid_score: 2.402623}      phone_brand_multi_layer_perceptron_1.bin
        (0.2, 128)      {[1825]	train_score: 2.401376	valid_score: 2.402689}      phone_brand_mlp_3.bin
        (0.2, 32)       {[1846]	train_score: 2.403960	valid_score: 2.403217}      phone_brand_multi_layer_perceptron_2

    device_model:
        (0.2, 64)       {[2652]	train_score: 2.369331	valid_score: 2.389789}      device_model_multi_layer_perceptron_1.bin
        (0.2, 32)       {[2891]	train_score: 2.374589	valid_score: 2.389791}
        (0.2, 128)      {[2288]	train_score: 2.368377	valid_score: 2.390303}

phone_brand:
    gbtree      [113]	train-mlogloss:2.396648	eval-mlogloss:2.402918
phone_brand_embedding_32
    gbtree      [51]	train-mlogloss:2.394280	eval-mlogloss:2.402758
phone_brand_embedding_64
    gbtree      [49]	train-mlogloss:2.393891	eval-mlogloss:2.4027
phone_brand_embedding_128
    gbtree      [52]	train-mlogloss:2.393246	eval-mlogloss:2.403260

installed_app:
    mlp (0.2, 64)       [657]	train_score: 2.247402	valid_score: 2.287982      installed_app_mlp_1.bin
    embedding from this model   installed_app_embedding_1
installed_app_label
    mlp (0.2, 64)       [687]	train_score: 2.308083	valid_score: 2.320131       installed_app_label_mlp_1.bin
    embedding from this model   installed_app_label_embedding_1


concat_6_embedding_64
    mlp (0.2, 64)       [296]	train_score: 2.210399	valid_score: 2.264480       concat_6_embedding_64_mlp_1.bin
                                submission score=2.25093
    mlp bypass (0.1, 64, 64)        [287]	train_score: 2.202804	valid_score: 2.261038
    mlp bypass (0.1, 64, 128)       [292]	train_score: 2.202365	valid_score: 2.260960   concat_6_embedding_64_mlp_3.bin
    mlp bypass (0.1, 128, 64)       [271]	train_score: 2.203536	valid_score: 2.261078
    mlp bypass (0.1, 128, 128)      [320]	train_score: 2.202656	valid_score: 2.261202

    based on concat_6_embedding_64_mlp_3.bin
    layer_sizes = [task.space, 64, 64, 128,  task.num_class]
    layer_inits = [('res:w0', 'res:b0'), ('res:pass', 'zero'), ('res:w1', 'res:b1'), ('res:w2', 'res:b2')]
    [0]	train_score: 2.213178	valid_score: 2.261509
    layer_sizes = [task.space, 64, 128, 128,  task.num_class]
    layer_inits = [('res:w0', 'res:b0'), ('res:pass', 'zero'), ('res:w1', 'res:b1'), ('res:w2', 'res:b2')]
    [0]	train_score: 2.215293	valid_score: 2.261441
    layer_sizes = [task.space, 64, 128, 128,  task.num_class]
    layer_inits = [('res:w0', 'res:b0'), ('res:w1', 'res:b1'), ('res:pass', 'zero'), ('res:w2', 'res:b2')]
    [0]	train_score: 2.213601	valid_score: 2.261232


    mlp (0.2, 128)      [232]	train_score: 2.206101	valid_score: 2.264820       concat_6_embedding_64_mlp_5.bin
    mlp bypass (0.1, 128, 128)      [251]	train_score: 2.195242	valid_score: 2.262748

    mlp (0.2, 32)       [329]	train_score: 2.225020	valid_score: 2.265452

concat_6_ooee_64
    mlp (0.2, 64)       [392]	train_score: 2.212981	valid_score: 2.267150



cocnat_21
    mlp{gd}
    1    (0.2, 64), batch_size=-1   , drops=(0.5, 1)     {[3076]train_score: 2.207119	valid_score: 2.258043}
    2    (0.2, 64), batch_size=10000, drops=(0.5, 1)     {[557]	train_score: 2.201392	valid_score: 2.257924}
    3    (0.2, 64), batch_size=10000, drops=(0.75,1)     {[441]	train_score: 2.198582	valid_score: 2.260310}
    4    (0.2, 64), batch_size=10000, drops=(0.25,1)     {[659]	train_score: 2.226501	valid_score: 2.260723}
    5    (0.2, 64), batch_size=1000 , drops=(0.5, 1)     {[56]	train_score: 2.207921	valid_score: 2.260046}
    6    (0.2, 64), batch_size=5000 , drops=(0.5, 1)     {[265]	train_score: 2.207076	valid_score: 2.258683}
    7    (0.2, 64), batch_size=15000, drops=(0.5, 1)     {[786]	train_score: 2.204690	valid_score: 2.259189}
    8    (0.2, 64), batch_size=20000, drops=(0.5, 1)     {[1074]train_score: 2.204936	valid_score: 2.258583}
    9    (0.1, 64), batch_size=1000 , drops=(0.5, 1)     {[118]	train_score: 2.195672	valid_score: 2.258665}
   10    (0.05,64), batch_size=1000 , drops=(0.5, 1)     {[203]	train_score: 2.206909	valid_score: 2.258858}
   11    (0.1, 64), batch_size=10000, drops=(0.5, 1)     {[1077]train_score: 2.200618	valid_score: 2.257873}      concat_21_mlp_1.bin
   12    (0.1,128), batch_size=10000, drops=(0.5, 1)     {[939]	train_score: 2.199193	valid_score: 2.258999}
    mlp{adam}
        (1e-4,64), batch_size=-1   , drops=(0.5, 1)     {[71]	train_score: 2.173218	valid_score: 2.256882}      submission score: 2.24364

一层：
    曲线大概已经对应不上了。单从结果来看。
    batch_size：bs越大，步长越小，需要训练的轮数越多，对收敛结果的影响不规则，不同bs造成的影响差距感觉更来源于参数初始化时候的不确定性，
                理由为在参数（0.2， 64， 10000）条件下，多次训练有时也无法达到2.2579+ ， 对比1,2,5,6,7,8. 2中结果最好，1次之
    drop out  :drops取个中间值感觉较好，对比2,3,4分别取值为0.5,0.25,0.75 其中取值0.5时效果较好，不同取值差距较大。
    learning rate：lr越小，需要训练的轮数越多。对比5,9,10大概是lr取小一点会稍微好一点
    神经元个数 ：对比11,12。64一直以来比128（以及32）好点
在用gd训练好后，用它的参数再取比较小lr用adam算法可以有一点点提升。


ensemble_6
    mlp{gd}
        (0.2, 64), batch_size=10000, drops=(0.5, 1)     {[154]	train_score: 2.117223	valid_score: 2.265209}      ensemble_6_mlp_1.bin
        (0.2, 64), batch_size=10000, drops=(0.5, 1)     {[150]	train_score: 2.120430	valid_score: 2.264654}
        (0.2, 64), batch_size=10000, drops=(0.5, 1)  layer_l2 = [0.001, 0.001]   {[176]	train_score: 2.100909	valid_score: 2.264528}
        (0.2, 64), batch_size=-1   , drops=(0.5, 1)     {电脑瘫痪，大概数据稠密，内存爆了}
        (0.2, 64), batch_size=20000, drops=(0.5, 1)     {[317]	train_score: 2.112188	valid_score: 2.264989}
        (0.1, 64), batch_size=4096 , drops=(0.5, 1)     {[302]	train_score: 2.122921	valid_score: 2.266524}
        (0.1, 64), batch_size=10000, drops=(0.75,1)     {[271]	train_score: 2.115626	valid_score: 2.266728}
        (0.1, 64), batch_size=10000, drops=(0.6, 1)     {[281]	train_score: 2.117615	valid_score: 2.265771}
        (0.1, 64), batch_size=10000, drops=(0.4, 1)     {[315]	train_score: 2.130092	valid_score: 2.264849}

    mlp{adam}
        (0.0001,100),batch_size=10000,drops=(0.3,1)  layer_l2 = [0.001, 0.001]      {[204]	train_score: 2.074832	valid_score: 2.261009}
        (0.0001,100),batch_size=10000,drops=(0.4,1)  layer_l2 = [0.001, 0.001]      {[170]	train_score: 2.088144	valid_score: 2.259334}
        (0.0001,100),batch_size=10000,drops=(0.5,1)  layer_l2 = [0.001, 0.001]      {[168]	train_score: 2.077618	valid_score: 2.259518}
        (0.0001,100),batch_size=10000,drops=(0.6,1)  layer_l2 = [0.001, 0.001]      {[159]	train_score: 2.075406	valid_score: 2.259630}
        (0.0001,100),batch_size=10000,drops=(0.7,1)  layer_l2 = [0.001, 0.001]      {[147]	train_score: 2.079877	valid_score: 2.259496}
        (0.0001,100),batch_size=10000,drops=(0.8,1)  layer_l2 = [0.001, 0.001]      {[151]	train_score: 2.064241	valid_score: 2.259866}
        (0.0001,100),batch_size=10000,drops=(0.9,1)  layer_l2 = [0.001, 0.001]      {[138]	train_score: 2.072581	valid_score: 2.259901}



concat_1
    mlp{adam}
        (0.0001, 64), batch_size=10000,drops=(0.5,1)  layer_l2 = [0.001, 0.001]     [1235]	train_score: 2.389359	valid_score: 2.393016
    mlp{gd}
        (0.2, 64), batch_size=10000, drops(0.5, 1)                                  [2240]	train_score: 2.371089	valid_score: 2.388144       concat_1_mlp_2.bin

concat_6_net2net
    (0.2, 128)   [85]	train_score: 2.076158	valid_score: 2.026671

    {gd} (0.2, 64)    [132]	train_score: 2.000493	valid_score: 2.004339
    在上面模型基础上adam，（0.0001,64）        [217]	train_score: 1.698932	valid_score: 1.951654       concat_6_net2net_mlp_11.bin


concat_1
    layer_l2 = [0.0001, 0.0001] (0.2, 64) batch_size = 1000     [283]	train_score: 2.369955	valid_score: 2.387958
    layer_l2 = [0.0001, 0.0001] (0.1, 64) batch_size = 1000     [574]	train_score: 2.367736	valid_score: 2.387548       concat_1_mlp_100.bin
    layer_l2 = [0.0001, 0.0001] (0.2, 128)batch_size = 1024     [243]	train_score: 2.367487	valid_score: 2.388407
    layer_l2 = [0.0001, 0.0001] (0.2, 64) batch_size = 1000     [537]	train_score: 2.370447	valid_score: 2.389186



    [228]	train_score: 2.363045	valid_score: 2.387705
    concat_1_mlp_2.bin  layer_l2 = [0.0001, 0.0001]  (64, 0.0001, -1) [320]	train_score: 2.359724	valid_score: 2.387345    score:2.38510
                                                                      [350] score: 2.38504
                                                                      concat_1_mlp_1001.bin


    concat_6_mlp_143.bin  layer_l2 = [0.0001, 0.0001] (128, 0.00001, -1)    concat_6_net2net_mlp_1000.bin

    [196]	train_score: 1.668376	valid_score: 1.943452
    [225]	loss: 1.683151 	train_score: 1.670080	valid_score: 1.942579

    [281]	loss: 1.678444 	train_score: 1.665124	valid_score: 1.943029
best iteration:
[262]	train_score: 1.665451	valid_score: 1.942998
[-1]	train_score: 1.665124	valid_score: 1.943029
training time elapsed: 239.330602
final train_score: 2.12850044731 final valid_score: 2.25332626642
model dumped at ../model/concat_7_norm_net2net_mlp_1000.bin

layer_sizes = [task.space, 128, task.num_class]
    layer_activates = ['relu', None]
    layer_inits = [('net2:w0', 'net2:b0'), ('net2:w1', 'net2:b1')]
    init_path = '../model/concat_6_mlp_143.bin'
    layer_drops = [0.5, 1]
    layer_l2 = [0.00001, 0.00001]
    opt_algo = 'adam'
    learning_rate = 0.00001
    batch_size = -1
    num_round = 3000
    early_stop_round = 20

    [134]	train_score: 1.675315	valid_score: 1.942419

    33436 55443 22113 44336 3343666 5544363


    0x0123  1.9410

    concat_24_freq

    concat_1_ensemble_1024   2.38472
