16/8/12 Fri.
feature
    # name, feature_type, space, rank
    phone_brand, one_hot, 131, 1
    device_model, one_hot, 1666, 1

    device_long_lat, multi, 10, 10              # mean, max, min, std, median ...
    _norm, multi, 10, 10         # scale long/lat to [0, 1]

    device_event_num, num, 1, 1
    _norm, ...
    device_day_event_num, multi, 31, 8
    _norm, ...
    device_hour_event_num, multi, 24, 24
    _norm, ...
    device_day_hour_event_num, multi, 744, 169
    _norm, ...

    installed_app, multi, 19237, 3446
    _freq, ...
    active_app, multi, 19237, 1342
    _freq, ...

    installed_app_label, multi, 507, 411
    _freq, ...
    active_app_label, multi, 507, 357
    _freq, ...

    emsemble_1, multi, 42215, 5890

model
    gblinear
        alpha, lambda

    gbtree
        max_depth, eta, subsample, colsample_bytree

    baba
        lr: finish
        fm: part
        nn (as proposed on forum)


submission
    # data, model, best_param, valid_score, submission_score, dumped_file
    gym
        concat_5, gbtree, {max_depth=3, eta=0.1, subsample=0.8, colsample=0.5, round=860, rate=0.2},
                                2.2824732, 2.27017, concat_5_gbtree_1
        concat_5, gblinear, {alpha=0.1, lambda=66, early_stop_round=1, round=3, rate=0.2},
                                2.29340173483, not yet, concat_5_gblinear_1
        concat_4, gbtree, {max_depth=3, eta=0.1, subsample=0.8, colsample=0.9, round=950, rate=0.2},
                                2.28721, 2.27817, concat_4_gbtree_1
        concat_4, gblinear, {alpha=0.05, lambda=44, early_stop_round=1, round=2, rate=0.2},
                                2.29869730537, not yet, concat_4_gblinear_1
        concat_4_norm, gbtree, {max_depth=3, eta=0.1, subsample=0.8, colsample=0.7, round=777, rate=0.2},
                                2.29613, not yet, concat_4_norm_gbtree_1
        concat_4_norm, gblinear, {alpha=0.1, lambda=13, early_stop_round=1, rate=0.2},
                                2.32331138572, not yet, concat_4_norm_gblinear_1

    rocky
        concat_1, gbtree            max_depth 7 eta 0.07 subsample 0.8 colsample_bytree 0.5 2.35875357389 2.39043890146
        concat_1, gblinear          alpha 0 lambda 7.5  2.3499658672 2.38944255486
        concat_2, gbtree            max_depth 3 eta 0.07 subsample 0.8 colsample_bytree 0.6 2.33399659152 2.38934798614
        concat_2, gblinear          alpha 0.001 lambda 8 2.34606485552 2.38613098915
        concat_2_norm, gbtree       max_depth 3 eta 0.07 subsample 0.8 colsample_bytree 0.6 2.33028431943 2.38669627018
        concat_2_norm, gblinear     alpha 0.001 lambda 7.5 2.34475563006 2.38551132474

    baba
        concat_5_norm, gbtree, {max_depth=4, eta=0.1, subsample=0.7, colsample=0.7}, [556]   train-mlogloss:1.89167	eval-mlogloss:2.28766, concat_5_norm_gbtree_1
        concat_5_norm, gblinear, {0.001, 10}, [2]	train-mlogloss:2.15605	eval-mlogloss:2.32268, concat_5_norm_gblinear_1
        concat_5_norm, lr, {0, 0}


2016/8/13 Sat.

feature:
    ensemble_1
    ensemble_2

model:
    baba:
        fm: finish
        mlp: finish

submission:
    baba:
        ensemble, average, {[ 0.01968983,  0.0391137 ,  0.01906632,  0.00886792,  0.0204471 ,
                                0.02016268,  0.25404595,  0.2369304 ,  0.22133675,  0.16033936,]}, 2.03700311,  2.28009707, average_1.log

    gym:
        ensemble_2, gbtree, {max_depth=2, eta=0.01, subsample=0.01, colsample=0.01, early_stop_round=50,
                                round=3980, lambda=19, alpha=0.1, rate=0.2}
                            valid_score=2.27704846522,  real_score=2.27079, ensemble_2_gbtree

        concat_1, factorization_machine, {learning_rate=0.15, l1_w=0.2, l1_v=0.1, num_round=300, batch_size=15,
                                early_stopping_round=10, } valid_score=2.35431 real_score=not yet

        concat_6, gbtree, {max_depth=3, eta=0.1, subsample=0.7, colsample=0.8, early_stop_round=25,
                                round=720, lambda=1, alpha=0.1, rate=0.2}
                             valid_score=2.27860747937, real_score=2.26798, concat_6_gbtree

        concat_7, gbtree, {max_depth=3, eta=0.1, subsample=0.8, colsample=0.9, early_stop_round=25,
                                round=673, lambda=1, alpha=0.1, rate=0.2}
                             valid_score=2.2813284161, real_score=not yet, concat_7_gbtree

        concat_10, gbtree, {max_depth=3, eta=0.1, subsample=0.6, colsample=0.8, early_stop_round=30,
                                round=512, lambda=1, alpha=0.1, rate=0.2}
                             valid_score=2.28447753216, real_score=not yet, concat_10_gbtree

2016/8/14 Sun.

2016/8/15 Mon.

feature:

model:
    baba:
        blending
        bagging

submission:

submission:
    baba:
        concat_6_mlp_1:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 0.5]
            learning_rate = 0.2
            num_round = 470
            471	2.205019	2.205045	2.258668
            score: 2.24483

    rocky:
        concat_6_mlp:
            opt_algo:
            drops:
            multi-layer:

    gym:
        mlp on other features

        concat_6_mlp_1:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 0.5]
            learning_rate = 0.2
            num_round = 500
            457	2.207887	2.207884	2.258564
            score:not yet

        concat_10_mlp_1:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 0.5]
            learning_rate = 0.2
            [475]	train_score: 2.202560	valid_score: 2.258602
            score:not yet




2016/8/16:

feature:
    baba:
        ensemble_3: [concat_1_gbtree_1, concat_1_gblinear_1, concat_6]
        ensemble_4: [concat_(1-5)_(norm)_gb(tree/linear)_1, concat_6]

model:

submission:
    baba:
        ensemble_3_mlp_1:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 0.5]
            learning_rate = 0.2
            345	2.184544	2.184539	2.257029
        ensemble_4_mlp_1

2016/8/17:
feature:
    gym:
        time_label_group:
        concat_11:
        concat_12:

model:
    baba:
        fnn

submission:
    rocky:
        concat_7_norm:
            gblinear:
            gbtree:
            mlp:

        random_forest:

        blending:

    baba:
        concat_6:
            fm:

        concat_7_norm:
            fm:

        concat_10_norm:
            fm:

    gym:
        ensemble_3_mlp_1:
            layer_sizes = [ti.SPACE, 90, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 0.5]
            learning_rate = 0.2
            num_round = 345
            327	2.189949	2.189951	2.257159
            score:2.24554

        concat_10_gblinear_1:
            alpha 0.1 lambda 45 2.16418639228 2.27902957404

        concat_10_mlp_1:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 0.5]
            learning_rate = 0.2
            num_round = 500
            461	2.206019	2.206028	2.258760

        concat_6_mpl_5:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 1]
            learning_rate = 0.2
            num_round = 500
            [498]	loss: 2.197575 	train_score: 2.197590	valid_score: 2.258964	time: 5

        concat_11_mlp_1:
            layer_sizes = [ti.SPACE, 100, ti.NUM_CLASS]
            layer_activates = ['relu', None]
            drops = [0.5, 1]
            learning_rate = 0.2
            num_round = 500
            [483]	train_score: 2.203397	valid_score: 2.258890

        concat_11, gbtree, {max_depth=3, eta=0.1, subsample=0.8, colsample=0.5, early_stop_round=20,
                                round=2000, lambda=1, alpha=0.1, rate=0.2}
                             valid_score=2.27878280383, real_score=not yet, concat_10_gbtree
