gbtree:
cv rate 0.75
Stopping. Best iteration:
[814]	train-mlogloss:2.074024	eval-mlogloss:2.281378

gblinear:
cv rate 0.75
lambda 40, alpha 0
Stopping. Best iteration:
[81]	train-mlogloss:2.157845	eval-mlogloss:2.275807

#################################################################

99 	new
xepa

2.27845
	3 	Mon, 25 Jul 2016 02:33:14
Your Best Entry â†‘
You improved on your best score by 0.00595.
You just moved up 39 positions on the leaderboard. Tweet this!

lambda 40, alpha 0.1
[131]	train-mlogloss:2.162140	eval-mlogloss:2.275910
Stopping. Best iteration:
[81]	train-mlogloss:2.162170	eval-mlogloss:2.275790

../model/xgb.model.0.75.2.gblinear.40.0.10 ../model/dunp.raw.txt.0.75.2.gblinear.40.0.10
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.2.gblinear.40.0.10

#################################################################


lambda 40, alpha 0.2
[128]	train-mlogloss:2.166329	eval-mlogloss:2.275923
Stopping. Best iteration:
[78]	train-mlogloss:2.166365	eval-mlogloss:2.275775

../model/xgb.model.0.75.2.gblinear.40.0.20 ../model/dunp.raw.txt.0.75.2.gblinear.40.0.20
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.2.gblinear.40.0.20



lambda 40, alpha 0.05
[132]	train-mlogloss:2.159988	eval-mlogloss:2.275931
Stopping. Best iteration:
[82]	train-mlogloss:2.160016	eval-mlogloss:2.275782

../model/xgb.model.0.75.1.gblinear.40.0.05 ../model/dunp.raw.txt.0.75.1.gblinear.40.0.05
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.1.gblinear.40.0.05


lambda 40, alpha 0.01
Stopping. Best iteration:
[86]	train-mlogloss:2.158261	eval-mlogloss:2.275812

../model/xgb.model.0.75.1.gblinear.40.0.01 ../model/dunp.raw.txt.0.75.1.gblinear.40.0.01
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.1.gblinear.40.0.01


###########################################################################################

concat_1_gblinear_1

alpha 0.3
34 2.15701532927 2.28234843885
35 2.15904372375 2.28233299089
36 2.16105479832 2.28233394403
alpha 0.4
34 2.16120692285 2.28225204592
35 2.16317302165 2.28232343339
36 2.16508772304 2.28229723916
alpha 0.5
34 2.16526266057 2.28223748662
35 2.16714892668 2.28226789307
36 2.16897828874 2.28232924003

2.27067

concat_1_gblinear_2

alpha 0.005
lambda 38 2.15247446334 2.28230528176
lambda 39 2.15450208394 2.28227293376
lambda 40 2.15646406472 2.28227095873
lambda 41 2.15836340996 2.28227811978
lambda 42 2.16021862039 2.28229322017
alpha 0.01
lambda 38 2.15270422129 2.2823219883
lambda 39 2.15471087477 2.2822671708
lambda 40 2.15665528648 2.28225370407
lambda 41 2.15856940276 2.28226824831
lambda 42 2.16040524476 2.28229304599
alpha 0.015
lambda 38 2.15290202915 2.28226700121
lambda 39 2.15491966652 2.28225999632
lambda 40 2.15687602414 2.28225008478
lambda 41 2.15877018528 2.28226126854
lambda 42 2.16061564744 2.28228095752

best param: alpha: 0.015, lambda: 40
score: 2.27085

concat_1_gbtree_1

gbtree:

max_depth
1 2.24363242858 2.29815467706
2 2.14364295158 2.28460162915
3 2.08373835726 2.28457981331
4 2.05670338119 2.28502381076
5 2.00977379022 2.28502305684

max_depth 2
eta 0 2.48490643501 2.48490643501
eta 0.1 2.14364295158 2.28460162915
eta 0.2 2.12742739571 2.2880187208
eta 0.3 2.15650267129 2.29145497671
max_depth 3
eta 0 2.48490643501 2.48490643501
eta 0.1 2.08373835726 2.28457981331
eta 0.2 2.09105607433 2.28844678095
eta 0.3 2.12342046676 2.29279467186

max_depth 2
subsample 0.5 2.13623017263 2.28541826344
subsample 0.6 2.13940002628 2.28569953928
subsample 0.7 2.14364295158 2.28460162915
subsample 0.8 2.15006506 2.28522573054
subsample 0.9 2.16001249303 2.28644298779
max_depth 3
subsample 0.5 2.08178309707 2.28567226622
subsample 0.6 2.09073736241 2.28453511635
subsample 0.7 2.08373835726 2.28457981331
subsample 0.8 2.0694756099 2.28333099061
subsample 0.9 2.08204467647 2.28288280954

max_depth 2
subsample 0.6 2.13940002628 2.28569953928
subsample 0.7 2.14364295158 2.28460162915
subsample 0.8 2.15006506 2.28522573054
subsample 0.9 2.16001249303 2.28644298779
subsample 1 2.17860011977 2.29112955327
max_depth 3
subsample 0.6 2.09073736241 2.28453511635
subsample 0.7 2.08373835726 2.28457981331
subsample 0.8 2.0694756099 2.28333099061
subsample 0.9 2.08204467647 2.28288280954
subsample 1 2.10200649962 2.28749881831

########################################################################################################

concat_2_gblinear_1

alpha 0
lambda 8 2.35057514063 2.39189004871
lambda 9 2.35310859652 2.39185100411
lambda 10 2.35531593723 2.391858509
lambda 11 2.35726340759 2.39189771086
alpha 0.0001
lambda 8 2.3505777335 2.39188963375
lambda 9 2.35310975962 2.39185132862
lambda 10 2.3553176807 2.39185870548
lambda 11 2.35726494912 2.39189768991
alpha 0.001
lambda 8 2.350593289 2.39188966793
lambda 9 2.35312490333 2.39185150797
lambda 10 2.35533098691 2.39185921833
lambda 11 2.35727825235 2.39189874913
alpha 0.005
lambda 8 2.35066418004 2.39189067375
lambda 9 2.35319018255 2.39185383282
lambda 10 2.35539259413 2.39186196543
lambda 11 2.35733616231 2.3919018078

best param: alpha: 0, lambda: 9
score: 2.38720

###############################################################################################

concat_3_gblinear_1

alpha 0
lambda 0 24.2138329194 24.9587750208
lambda 0.01 1.89483430485 2.49486394454
lambda 0.1 2.07696736119 2.37026414056
lambda 1 2.22148216055 2.34905041049
alpha 0.001
lambda 0 23.9932073528 24.5890512134
lambda 0.01 1.89772155812 2.49212964383
lambda 0.1 2.07765953121 2.37003976579
lambda 1 2.22164210299 2.34905865989
alpha 0.01
lambda 0 24.2273522898 24.8972928942
lambda 0.01 1.92049718912 2.47395908529
lambda 0.1 2.0836805034 2.36811535005
lambda 1 2.22304964097 2.34913707815
alpha 0.1
lambda 0 2.01258524994 2.43929636442
lambda 0.01 2.04663726822 2.39085353032
lambda 0.1 2.12485197859 2.35482525923
lambda 1 2.2348752376 2.34997135761

#######################################################################################################

concat_6_gblinear_1
alpha 0.007
lambda 13 2.18204718974 2.30774710531
lambda 14 2.18734304288 2.30773609643
lambda 15 2.19219201669 2.30776665034
alpha 0.008
lambda 13 2.18211466168 2.30776142001
lambda 14 2.18740219592 2.3077251923
lambda 15 2.19225589513 2.3077682522
alpha 0.009
lambda 13 2.18217765468 2.30775869686
lambda 14 2.18746709287 2.30771381398
lambda 15 2.19230970055 2.30777273839
alpha 0.01
lambda 13 2.18224207615 2.30774687067
lambda 14 2.18752644804 2.30772384991
lambda 15 2.19236214926 2.30776886799
alpha 0.02
lambda 13 2.1828822231 2.30776362623
lambda 14 2.18813937361 2.30774112097
lambda 15 2.19295304299 2.30779223504

best param: alpha 0.009, lambda 14
score: 2.29605