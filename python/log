gbtree:
cv rate 0.75
Stopping. Best iteration:
[814]	train-mlogloss:2.074024	eval-mlogloss:2.281378

gblinear:
cv rate 0.75
lambda 40, alpha 0
Stopping. Best iteration:
[81]	train-mlogloss:2.157845	eval-mlogloss:2.275807

#################################################################

99 	new
xepa

2.27845
	3 	Mon, 25 Jul 2016 02:33:14
Your Best Entry â†‘
You improved on your best score by 0.00595.
You just moved up 39 positions on the leaderboard. Tweet this!

lambda 40, alpha 0.1
[131]	train-mlogloss:2.162140	eval-mlogloss:2.275910
Stopping. Best iteration:
[81]	train-mlogloss:2.162170	eval-mlogloss:2.275790

../model/xgb.model.0.75.2.gblinear.40.0.10 ../model/dunp.raw.txt.0.75.2.gblinear.40.0.10
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.2.gblinear.40.0.10

#################################################################


lambda 40, alpha 0.2
[128]	train-mlogloss:2.166329	eval-mlogloss:2.275923
Stopping. Best iteration:
[78]	train-mlogloss:2.166365	eval-mlogloss:2.275775

../model/xgb.model.0.75.2.gblinear.40.0.20 ../model/dunp.raw.txt.0.75.2.gblinear.40.0.20
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.2.gblinear.40.0.20



lambda 40, alpha 0.05
[132]	train-mlogloss:2.159988	eval-mlogloss:2.275931
Stopping. Best iteration:
[82]	train-mlogloss:2.160016	eval-mlogloss:2.275782

../model/xgb.model.0.75.1.gblinear.40.0.05 ../model/dunp.raw.txt.0.75.1.gblinear.40.0.05
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.1.gblinear.40.0.05


lambda 40, alpha 0.01
Stopping. Best iteration:
[86]	train-mlogloss:2.158261	eval-mlogloss:2.275812

../model/xgb.model.0.75.1.gblinear.40.0.01 ../model/dunp.raw.txt.0.75.1.gblinear.40.0.01
112071x40270 matrix with 2133249 entries is loaded from ../input/test.csv
../output/submission.csv.0.75.1.gblinear.40.0.01


###########################################################################################

2016.8.7

gblinear

alpha:0 lambda:0
[0]	train-mlogloss:1.834446	eval-mlogloss:2.568943

alpha:0 lambda:10
[58]	train-mlogloss:2.030013	eval-mlogloss:2.302813

alpha:0 lambda:25
[52]	train-mlogloss:2.117742	eval-mlogloss:2.284111

alpha:0 lambda:30
[65]	train-mlogloss:2.133154	eval-mlogloss:2.282907

alpha:0 lambda:35
[54]	train-mlogloss:2.145712	eval-mlogloss:2.282403

alpha:0 lambda:40
[46]	train-mlogloss:2.156202	eval-mlogloss:2.282362

alpha:0 lambda:45
[53]	train-mlogloss:2.165180	eval-mlogloss:2.282633

alpha:0 lambda:50
[47]	train-mlogloss:2.173005	eval-mlogloss:2.283091

alpha:0 lambda:75
[36]	train-mlogloss:2.201352	eval-mlogloss:2.286622

alpha:0 lambda:100
[38]	train-mlogloss:2.219946	eval-mlogloss:2.290718

alpha:0 lambda:1000
[7]	train-mlogloss:2.340345	eval-mlogloss:2.355150

alpha 0.3
34 2.15701532927 2.28234843885
35 2.15904372375 2.28233299089
36 2.16105479832 2.28233394403
alpha 0.4
34 2.16120692285 2.28225204592
35 2.16317302165 2.28232343339
36 2.16508772304 2.28229723916
alpha 0.5
34 2.16526266057 2.28223748662
35 2.16714892668 2.28226789307
36 2.16897828874 2.28232924003

early stop:
2.0962164431 2.28881448417

gbtree:

max_depth
1 2.24363242858 2.29815467706
2 2.14364295158 2.28460162915
3 2.08373835726 2.28457981331
4 2.05670338119 2.28502381076
5 2.00977379022 2.28502305684

max_depth 2
eta 0 2.48490643501 2.48490643501
eta 0.1 2.14364295158 2.28460162915
eta 0.2 2.12742739571 2.2880187208
eta 0.3 2.15650267129 2.29145497671
max_depth 3
eta 0 2.48490643501 2.48490643501
eta 0.1 2.08373835726 2.28457981331
eta 0.2 2.09105607433 2.28844678095
eta 0.3 2.12342046676 2.29279467186

max_depth 2
subsample 0.5 2.13623017263 2.28541826344
subsample 0.6 2.13940002628 2.28569953928
subsample 0.7 2.14364295158 2.28460162915
subsample 0.8 2.15006506 2.28522573054
subsample 0.9 2.16001249303 2.28644298779
max_depth 3
subsample 0.5 2.08178309707 2.28567226622
subsample 0.6 2.09073736241 2.28453511635
subsample 0.7 2.08373835726 2.28457981331
subsample 0.8 2.0694756099 2.28333099061
subsample 0.9 2.08204467647 2.28288280954

max_depth 2
subsample 0.6 2.13940002628 2.28569953928
subsample 0.7 2.14364295158 2.28460162915
subsample 0.8 2.15006506 2.28522573054
subsample 0.9 2.16001249303 2.28644298779
subsample 1 2.17860011977 2.29112955327
max_depth 3
subsample 0.6 2.09073736241 2.28453511635
subsample 0.7 2.08373835726 2.28457981331
subsample 0.8 2.0694756099 2.28333099061
subsample 0.9 2.08204467647 2.28288280954
subsample 1 2.10200649962 2.28749881831

